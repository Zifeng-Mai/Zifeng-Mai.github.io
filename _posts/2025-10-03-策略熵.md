---
layout:     post
title:      强化学习中的熵 (1)
subtitle:   策略熵
date:       2025-10-03
author:     Zifeng Mai
header-img: img/enoshima.jpg
preview:    本文探讨了强化学习中的策略熵 (policy entropy)，主要从理论上回答了两大问题：1. 为什么熵减往往意味着训练收敛；2. 策略梯度算法如何改变策略熵。从这两个问题的答案我们可以看出，策略梯度算法会导致策略熵的减小，从而使得策略收敛下来，这给了我们一个新的视角来看待强化学习的训练过程。
catalog: true
tags:
    - Reinforcement Learning
    - Policy (Shannon) Entropy
    - Renyi Entropy
    - Policy Gradient
---

## 引言

策略熵 (policy entropy) 是强化学习中非常重要的一个概念，它衡量了当前策略决策时的不确定性。

对于离散的动作空间 $\mathcal{A}$，策略 $\pi$ 在状态 $s$ 的熵定义为：

$$
\begin{equation}
\begin{aligned}
\mathcal{H}(\pi(\cdot\mid s))
&:=\mathbb{E}_{a\sim\pi(\cdot\mid s)}[-\log \pi(a\mid s)]\\
&=-\frac{1}{|\mathcal{A}|}\sum_{a\in\mathcal{A}}\pi(a\mid s)\log \pi(a\mid s)
\end{aligned}
\end{equation}
$$

进一步地，如果 $\pi$ 是一个softmax policy，则

$$
\begin{equation}
\pi(a\mid s)=\frac{\exp(z_\theta(s,a))}{\sum_{a'\in\mathcal{A}}\exp(z_\theta(s,a'))}
\end{equation}
$$

其中，$z_\theta(s,a)$ 是logits系数。

在利用强化学习（特别是PG算法）训练一个softmax策略时，很容易出现熵坍塌的现象，即在训练早期策略熵很快收敛，导致采样效率很低，难以获得有效的监督信号进一步提升能力。

本文将从两个方面论证这一点：

1. 为什么熵减往往意味着训练收敛
2. 策略梯度更新如何改变策略熵



## 一、为什么熵减意味着训练收敛

> Reference
>
> - https://zhuanlan.zhihu.com/p/1950579532802270647
> - 《Harnessing Uncertainty: Entropy-Modulated Policy Gradients for Long-Horizon LLM Agents》

**Theorem 1.** 给定一个softmax策略 $\pi$，有如下结论成立

$$
\begin{equation}
\mathbb{E}_{a\sim\pi_\theta(\cdot\mid s)}[\|\nabla_{z_\theta(s,a)}\log\pi_{\theta}(a\mid s)\|_2^2]=1-\exp(H_2(\pi))
\end{equation}
$$

其中，$H_2(\pi)$ 表示 $\pi$ 的2阶Renyi熵。

公式$(1)$中定义的熵称为香农熵，也是1阶Renyi熵。由于Renyi熵关于阶数是单调不增的，即$\mathcal{H}(\pi)\ge H_2(\pi)$，因此我们有

$$
\begin{equation}
\mathbb{E}_{a\sim\pi_\theta(\cdot\mid s)}[\|\nabla_{z_\theta(s,a)}\log\pi_{\theta}(a\mid s)\|_2^2]\le1-\exp(\mathcal{H}(\pi))
\end{equation}
$$

定理1说明了，一个softmax策略在状态 $s$ 处的策略梯度的范数与策略熵的变化方向相同。即当策略熵越小，策略梯度的范数就越小，也就是训练趋于收敛。



下面的定理2从另外一个角度说明了这个事情。

**Theorem 2.** 假设策略 $\pi_\theta$ 经过一次更新后，得到新策略 $\pi_{\theta^+}$。这两个策略的KL散度存在上界：

$$
\begin{equation}
\begin{aligned}
KL(\pi_{\theta^+}(s);\pi_{\theta}(s))\le&\frac{|\mathcal{A}|}{2}\cdot \|\Delta_s\|_\infty^2\cdot(1-\exp(-\mathcal{H}(\pi_\theta(s))))\\\
&+o\left(\|\Delta_s\|_2^2\right)
\end{aligned}
\end{equation}
$$

其中 $\Delta_s$ 表示两个策略在同一个状态 $s$ 处的logits向量之差，即

$$
\begin{equation}
\Delta_s:=z_{\theta^+}(s)-z_\theta(s)
\end{equation}
$$

观察公式$(5)$右侧的第一项，我们有如下结论：

1. logits差的无穷范数（即logits变化最大的action）越小，新旧策略整体的KL变化幅度越小。
2. 当策略熵接近0，新旧策略的差异也接近0。



在附录中展示了这两个定理的详细证明，有兴趣的读者可以自行查看。



## 二、熵动力学分析

>Reference: 《The Entropy Mechanism of Reinforcement Learning for Reasoning Language Models》

在这一章中，我们定量分析了策略熵在一次梯度更新前后的变化量。最终，我们能得到下面两个结论：

1. 对于softmax策略，策略熵的变化取决于每个动作的【对数概率】和【logits变化量】的协方差
2. 如果策略是按照PG算法来更新的，则【logits变化量】正比于每个动作的【优势】

用更通俗的语言来表述上面两个结论就是：

- 高概率 + 高优势 动作 --> 熵减
- 低概率 + 高优势 动作 --> 熵增

这是符合直觉的。因为熵表示了模型的自信程度，如果模型发现一个低概率的动作能获得很高的优势，他会发现自己做错了，也就变得更不自信。

下面我们用更加形式化的语言来描述这两个结论。

**Theorem 3 (softmax策略熵的变化).**  给定一个softmax策略 $\pi$，在相邻两次的更新步中，二者的熵的变化量满足下面的一阶近似：

$$
\begin{equation}
\begin{aligned}
\mathcal{H}(\pi^{k+1})-\mathcal{H}(\pi^{k})
&= \mathbb{E}_s[\mathcal{H}(\pi^{k+1}\mid s)-\mathcal{H}(\pi^{k}\mid s)]\\
&\approx \mathbb{E}_s[-\textrm{Cov}_{a\sim \pi^k(s)}(\log\pi^k(a\mid s),z^{k+1}_{s,a}-z^{k}_{s,a})]\\
\end{aligned}
\end{equation}
$$

**Theorem 4 (PG算法下的策略熵变化).** 在定理3的条件下，若按照策略梯度算法进行更新，则有如下进一步的结论：

$$
\begin{equation}
\begin{aligned}
\mathcal{H}(\pi^{k+1})-\mathcal{H}(\pi^{k})
&\approx -\eta\cdot\textrm{Cov}_{a\sim \pi^k(s)}(\log\pi^k(a\mid s),\pi^k(a\mid s)\cdot A(s,a))\\
\end{aligned}
\end{equation}
$$

其中，$A(s,a)$ 表示动作 $a$ 在状态 $s$ 下的优势。



## 附录

### 定理1的证明

**Definition (Renyi熵).** 对于离散随机变量 $X$，设其概率分布为 $P(X=x_1)=p_i$，则 $\alpha$ 阶Renyi熵的定义为：

$$
\begin{equation}
H_\alpha(P)=\frac{1}{1-\alpha}\log\left( \sum_{i=1}^np_i^\alpha \right)
\end{equation}
$$

不难发现，香农熵就是在 $\alpha\rightarrow1$时的Renyi熵，即$\mathcal{H}(P)=\lim_{\alpha\rightarrow1}H_\alpha(P)$。

**Proof of Theorem 1.**

简化一下定理1中的符号表达，我们希望证明的是如下等式成立：

$$
\begin{equation}
\begin{aligned}
\mathbb{E}_{a_k\sim\pi}[\|\nabla\log\pi_k\|^2_2]
&=1-\exp(-H_2(\pi))\\
&=1-\sum_{j=1}^{|\mathcal{A}|}\pi_j^2
\end{aligned}
\end{equation}
$$

其中，$\pi$ 是一个softmax策略，即：

$$
\begin{equation}
\pi_k=\frac{\exp(z_k)}{\sum_{j}\exp(z_j)}
\end{equation}
$$

我们首先推导得到梯度策略的范数的表达式。考察单个动作 $a_k$ 策略梯度我们有：

$$
\begin{equation}
\begin{aligned}
\frac{\partial \log\pi_k}{\partial z_i}
&=\frac{\partial }{\partial z_i}\left[z_k-\log\left( \sum_j\exp(z_j) \right)\right]\\
&=\mathbb{I}(i=k)-\pi_i
\end{aligned}
\end{equation}
$$

因此，我们有

$$
\begin{equation}
\begin{aligned}
\|\nabla\log\pi_k\|^2_2
&=\sum_{i=1}^{|\mathcal{A}|}\left(\frac{\partial \log\pi_k}{\partial z_i}\right)^2\\
&=\sum_{i=1}^{|\mathcal{A}|}(\mathbb{I}(i=k)-\pi_i)^2\\
&=(1-\pi_k)^2+\sum_{i\neq k}\pi_i^2\\
&=1-2\pi_k+\sum_{j=1}^{|\mathcal{A}|}\pi_j^2
\end{aligned}
\end{equation}
$$

对所有动作求期望得：

$$
\begin{equation}
\begin{aligned}
\mathbb{E}_{a_k\sim\pi}[\|\nabla\log\pi_k\|^2_2]
&=\sum_{k=1}^{|\mathcal{A}|}\pi_k\|\nabla\log\pi_k\|^2_2\\
&=\sum_{k=1}^{|\mathcal{A}|}\pi_k\left(1-2\pi_k+\sum_{j=1}^{|\mathcal{A}|}\pi_j^2\right)\\
&=\sum_{k=1}^{|\mathcal{A}|}\pi_k-2\sum_{k=1}^{|\mathcal{A}|}\pi_k^2+\sum_{k=1}^{|\mathcal{A}|}\pi_k\left(\sum_{j=1}^{|\mathcal{A}|}\pi_j^2\right)\\
&=\sum_{k=1}^{|\mathcal{A}|}\pi_k-2\sum_{k=1}^{|\mathcal{A}|}\pi_k^2+\left(\sum_{j=1}^{|\mathcal{A}|}\pi_j^2\right)\left(\sum_{k=1}^{|\mathcal{A}|}\pi_k\right)\\
&=1-2\sum_{k=1}^{|\mathcal{A}|}\pi_k^2+\sum_{j=1}^{|\mathcal{A}|}\pi_j^2\\
&=1-\sum_{j=1}^{|\mathcal{A}|}\pi_j^2
\end{aligned}
\end{equation}
$$

证明完毕。

### Renyi熵单调性的证明

**Lemma (Renyi熵的阶数单调性).** Renyi熵关于阶数 $\alpha$ 是单调不减的，即

$$
\begin{equation}
\alpha<\beta\Rightarrow H_\alpha(P)\ge H_\beta(P)
\end{equation}
$$

**Proof of Lemma.** 我们定义如下的对数矩生成函数

$$
\begin{equation}
\psi(\alpha):=\log\left( \sum_{i=1}^np_i^\alpha \right)
\end{equation}
$$

因此Renyi熵可以表示为：$H_\alpha(P)=\frac{\psi(\alpha)}{1-\alpha},\alpha\neq1$。

下面我们证明 $\psi(\alpha)$ 是一个凸函数。令 $Z(\alpha)=\sum_{i=1}^np_i^\alpha$，我们有

$$
\begin{equation}
\begin{aligned}
\psi'(\alpha)&=\frac{Z'(\alpha)}{Z(\alpha)}\\
\psi''(\alpha)&=\frac{Z''(\alpha)Z(\alpha)-(Z'(\alpha))^2}{Z^2(\alpha)}\\
&=\frac{\sum_i p_i^\alpha(\log p_i)^2}{Z(\alpha)} - \left(\frac{\sum_i p_i^\alpha\log p_i}{Z(\alpha)} \ \right)^2\\
&\ge 0
\end{aligned}
\end{equation}
$$

最后一步的不等式是利用了Jensen不等式。

凸函数的一个重要性质为：割线斜率关于右端点单调不减，即对于任意的 $a<b<c$，我们有

$$
\begin{equation}
\frac{\psi(b)-\psi(a)}{b-a}\le\frac{\psi(c)-\psi(a)}{c-a}\le\frac{\psi(c)-\psi(b)}{c-b}
\end{equation}
$$

对于任意的$\alpha\neq 1$，我们定义切线斜率：

$$
\begin{equation}
s(\alpha):=\frac{\psi(\alpha)-\psi(1)}{\alpha-1}=\frac{\psi(\alpha)}{\alpha-1}=-H_\alpha(P)\\
\end{equation}
$$

则 $s(\alpha)$ 是一个单调递增函数，因此 $H_\alpha(P)$ 单调递减。

### 定理2的证明

首先简化一下记号。我们记策略向量 $\pi_s:=\pi_{\theta}(s),\pi_s^+:=\pi_{\theta^+}(s)$。定义向量函数 $f(z)=KL(\textrm{softmax}(z),\pi_s)$。

将 $f$ 在$z=z_s$ 处进行二阶泰勒展开，我们有：

$$
\begin{equation}
\begin{aligned}
KL(\pi_s^+;\pi_s)
&=f(z_s^+)\\
&=f(z_s)+\nabla_z\left( f(z_s) \right)^T(z_s^+-z_s)+\frac{1}{2}(z_s^+-z_s)^T\nabla_z^2 f(z_s)(z_s^+-z_s)+o\left( \|z_s^+-z_s\|_2^2 \right)\\
&=f(z_s)+\nabla_z\left( f(z_s) \right)^T\Delta_s+\frac{1}{2}\Delta_s^T\nabla_z^2 f(z_s)\Delta_s+o\left( \|\Delta_s\|_2^2 \right)\\
\end{aligned}
\end{equation}
$$

其中 $\Delta_s=z_s^+-z_s$。

关于函数 $f$，我们有如下三个等式成立：

$$
\begin{equation}
\begin{aligned}
f(z_s)&=0\\
\nabla_zf(z_s)&=0\\
\nabla_z^2f(z_s)&=\mathcal{F}(z_s)
\end{aligned}
\end{equation}
$$

其中，$\mathcal{F}$ 是策略在logits向量 $z$ 处的Fisher信息矩阵，即：

$$
\begin{equation}
\mathcal{F}(z):=\mathbb{E}_{a\sim p_z}\left[ \nabla_z\log p_z(a) \nabla_z\log p_z(a) ^T\right]
\end{equation}
$$

其中 $p_z(a)=\exp(z_a)/\sum_b\exp(z_b)$。

为了证明逻辑的连续性，我们在这里不具体证明这三个等式。具体的证明已经放到最后。

将上述的等式代入泰勒展开中，得到：

$$
\begin{equation}
\begin{aligned}
KL(\pi_s^+;\pi_s)
&=\frac{1}{2}\Delta_s^T\mathcal{F}(z_s)\Delta_s+o\left( \|\Delta_s\|_2^2 \right)\\
\end{aligned}
\end{equation}
$$

其中，

$$
\begin{equation}
\begin{aligned}
\Delta_s^T\mathcal{F}(z_s)\Delta_s
&=\Delta_s^T\mathbb{E}_{a\sim \pi_s}\left[ \nabla_z\log \pi_s(a) \nabla_z\log \pi_s(a) ^T\right]\Delta_s\\
&=\mathbb{E}_{a\sim \pi_s}\left[\Delta_s^T \nabla_z\log \pi_s(a) \nabla_z\log \pi_s(a) ^T \Delta_s\right]\\
&=\mathbb{E}_{a\sim \pi_s}\left[\left(\nabla_z\log \pi_s(a) ^T \Delta_s\right)^2\right]\\
&=\mathbb{E}_{a\sim \pi_s}\left[\left(\sum_{b\in\mathcal{A}}\frac{\partial \log\pi_s(a)}{\partial z_s(b)} \Delta_s(b)\right)^2\right]\\
&\le |\mathcal{A}|\cdot \mathbb{E}_{a\sim \pi_s}\left[\sum_{b\in\mathcal{A}}\left(\frac{\partial \log\pi_s(a)}{\partial z_s(b)} \Delta_s(b)\right)^2\right]\\
&\le |\mathcal{A}|\cdot \left(\max_{b\in\mathcal{A}}\Delta_s(b)^2 \right)\cdot\mathbb{E}_{a\sim \pi_s}\left[\sum_{b\in\mathcal{A}}\left(\frac{\partial \log\pi_s(a)}{\partial z_s(b)}\right)^2\right]\\
&= |\mathcal{A}|\cdot \|\Delta_s\|_\infty^2\cdot \mathbb{E}_{a\sim \pi_s}\left[\| \nabla_z\log \pi_s(a) \|_2^2\right]\\
&\le|\mathcal{A}|\cdot \|\Delta_s\|_\infty^2\cdot(1-\exp(-\mathcal{H}(\pi_s))
\end{aligned}
\end{equation}
$$

其中第一个不等式是利用了 Cauchy-Schwarz 不等式，最后一个不等号是定理1的结论。

至此，定理2得证。

**一阶导数证明**

对 $f$ 的形式做等价变形得：

$$
\begin{equation}
\begin{aligned}
f(z)
&=KL(p_z,\pi_s)\\
&=\sum_a p_z(a)\log\frac{p_z(a)}{\pi_s(a)}\\
&=\sum_a p_z(a)\log p_z(a) - \sum_a p_z(a)\log \pi_s(a)
\end{aligned}
\end{equation}
$$

对 $z_k$ 求偏导得：

$$
\begin{equation}
\begin{aligned}
\frac{\partial f}{\partial z_k}(z)
&=\sum_a \frac{\partial p_z(a)}{\partial z_k}\cdot\log p_z(a) + \sum_a \log p_z(a)\cdot\frac{1}{\log p_z(a)}\cdot \frac{\partial p_z(a)}{\partial z_k} - \sum_a\frac{\partial p_z(a)}{\partial z_k}\cdot\log \pi_s(a)\\
&=\sum_a \frac{\partial p_z(a)}{\partial z_k}(\log p_z(a)+1-\log\pi_s(a))\\
&=\sum_a \frac{\partial p_z(a)}{\partial z_k}(\log p_z(a)-\log\pi_s(a)) + \sum_a \frac{\partial p_z(a)}{\partial z_k}\\
&=\sum_a \frac{\partial p_z(a)}{\partial z_k}\log \frac{p_z(a)}{\pi_s(a)}+  \frac{\partial }{\partial z_k} \sum_a p_z(a)\\
&=\sum_a \frac{\partial p_z(a)}{\partial z_k}\log \frac{p_z(a)}{\pi_s(a)}
\end{aligned}
\end{equation}
$$

当 $z=z_s$ 时，$p_z(a)=\pi_s(a)$，因此 $\frac{\partial f}{\partial z_k}(z_s)=0$ 得证。



**二阶导数证明**

一个经典结论是：KL散度 $KL(p_z,p_{z_0})$ 在 $z=z_0$ 处的Hessian矩阵等于Fisher信息矩阵。下面我们证明这一点。

对一阶导数再次求导得：

$$
\begin{equation}
\begin{aligned}
\frac{\partial^2 f}{\partial z_k\partial z_l}(z)
&=\sum_a \frac{\partial^2 p_z(a)}{\partial z_k\partial z_l}\log \frac{p_z(a)}{\pi_s(a)} + \sum_a \frac{\partial p_z(a)}{\partial z_k}\cdot \frac{1}{p_z(a)}\cdot \frac{\partial p_z(a)}{\partial z_l}
\end{aligned}
\end{equation}
$$

代入 $z=z_s$ 得

$$
\begin{equation}
\begin{aligned}
\frac{\partial^2 f}{\partial z_k\partial z_l}(z_s)
&=\sum_a \frac{\partial \pi_s(a)}{\partial z_k}\cdot \frac{1}{\pi_s(a)}\cdot \frac{\partial \pi_s(a)}{\partial z_l}\\
&=\sum_a \pi_s(a)\frac{\partial \log\pi_s(a)}{\partial z_k}\cdot \frac{1}{\pi_s(a)}\cdot \pi_s(a)\frac{\partial \log\pi_s(a)}{\partial z_l}\\
&=\sum_a \pi_s(a)\cdot\frac{\partial \log\pi_s(a)}{\partial z_k}\cdot \frac{\partial \log\pi_s(a)}{\partial z_l}\\
&=\mathbb{E}_{a\sim \pi_s}\left[ \nabla_z\log \pi_s(a) \nabla_z\log \pi_s(a) ^T\right]\\
&=\mathcal{F}(z_s)
\end{aligned}
\end{equation}
$$

得证。

### 定理3的证明

对策略熵 $\mathcal{H}(\pi\mid s)$ 在 $\pi=\pi^k$ 处进行一阶泰勒展开，并代入 $\pi=\pi^{k+1}$ 得：

$$
\begin{equation}
\mathcal{H}(\pi^{k+1}\mid s)
\approx \mathcal{H}(\pi^{k}\mid s) + \langle \nabla \mathcal{H}(\pi^{k}\mid s),(z^{k+1}-z^k) \rangle
\end{equation}
$$

下面我们推导 $\nabla \mathcal{H}(\pi\mid s)$ 的形式：

$$
\begin{equation}
\begin{aligned}
\nabla\mathcal{H}(\pi\mid s)
&=\nabla \left( -\mathbb{E}_{a\sim \pi(\cdot\mid s)}[\log\pi(a\mid s)] \right)\\
&=-\mathbb{E}_{a\sim \pi(\cdot\mid s)}[\nabla \log\pi(a\mid s)+\log\pi(a\mid s)\nabla\log\pi(a\mid s)]\\
&=-\mathbb{E}_{a\sim \pi(\cdot\mid s)}[\log\pi(a\mid s)\nabla\log\pi(a\mid s)]\\
\end{aligned}
\end{equation}
$$

代入上式得

$$
\begin{equation}
\begin{aligned}
\mathcal{H}(\pi^{k+1}\mid s)-\mathcal{H}(\pi^{k}\mid s)
&\approx \langle \nabla \mathcal{H}(\pi^{k}\mid s),(z^{k+1}-z^k)\rangle\\
&= -\langle \mathbb{E}_{a\sim \pi^{k}(\cdot\mid s)}[\log\pi^{k}(a\mid s)\nabla\log\pi^{k}(a\mid s)],(z^{k+1}_{s,a}-z^k_{s,a})  \rangle\\
&= -\mathbb{E}_{a\sim \pi^{k}(\cdot\mid s)}[\log\pi^{k}(a\mid s)\langle\nabla\log\pi^{k}(a\mid s),(z^{k+1}_{s,a}-z^k_{s,a}) \rangle] \\
&= -\mathbb{E}_{a\sim \pi^{k}(\cdot\mid s)}\left[\log\pi^{k}(a\mid s) \sum_{a'\in\mathcal{A}} \frac{\partial \log \pi^k(a\mid s)}{\partial z_{s,a}} \cdot (z^{k+1}_{s,a'}-z^k_{s,a'}) \right]  \\
&= -\mathbb{E}_{a\sim \pi^{k}(\cdot\mid s)}\left[\log\pi^{k}(a\mid s) \sum_{a'\in\mathcal{A}} (\mathbb{I}(a=a')-\pi^k(a'\mid s)) \cdot (z^{k+1}_{s,a'}-z^k_{s,a'}) \right]  \\
&= -\mathbb{E}_{a\sim \pi^{k}(\cdot\mid s)}\left[\log\pi^{k}(a\mid s)\left[ (z^{k+1}_{s,a}-z^k_{s,a})- \sum_{a'\in\mathcal{A}} \pi^k(a'\mid s) \cdot (z^{k+1}_{s,a'}-z^k_{s,a'}) \right]\right]  \\
&=-\mathbb{E}_{a\sim \pi^{k}(\cdot\mid s)}\left[\log\pi^{k}(a\mid s) (z^{k+1}_{s,a}-z^k_{s,a})\right]+ \mathbb{E}_{a\sim \pi^{k}(\cdot\mid s)}\left[\log\pi^k(a\mid s)\cdot\mathbb{E}_{a'\sim \pi^{k}(\cdot\mid s)}[z^{k+1}_{s,a'}-z^k_{s,a'}] \right]   \\
&=-\mathbb{E}_{a\sim \pi^{k}(\cdot\mid s)}\left[\log\pi^{k}(a\mid s) (z^{k+1}_{s,a}-z^k_{s,a})\right]+ \mathbb{E}_{a\sim \pi^{k}(\cdot\mid s)}\left[\log\pi^k(a\mid s)\right]\cdot\mathbb{E}_{a'\sim \pi^{k}(\cdot\mid s)}[z^{k+1}_{s,a'}-z^k_{s,a'}]    \\
&= -\textrm{Cov}_{a\sim \pi^k(s)}(\log\pi^k(a\mid s),z^{k+1}_{s,a}-z^{k}_{s,a})\\
\end{aligned}
\end{equation}
$$

得证。

### 定理4的证明

在定理3的基础上，我们只需要证明以下引理，即可得到定理4。

**Lemma.** 给定一个softmax策略，并按照PG算法进行一步更新，则 $z_{s,a}$ 的变化量满足：

$$
\begin{equation}
z^{k+1}_{s,a}-z^{k}_{s,a}=\eta\cdot\pi^k(a\mid s)\cdot A(s,a)
\end{equation}
$$

**Proof of Lemma.**

$$
\begin{equation}
\begin{aligned}
z^{k+1}_{s,a}-z^{k}_{s,a}
&=\eta\cdot\nabla_{z_{s,a}} J(\theta)\\
&=\eta\cdot\mathbb{E}_{a'\sim\pi^k(s)}\left[ \nabla_{z_{s,a}}\log\pi^k(a'\mid s)\cdot A(s,a') \right]\\
&=\eta\cdot\mathbb{E}_{a'\sim\pi^k(s)}\left[ \frac{\partial \log\pi^k(a'\mid s)}{\partial z_{s,a}} \cdot A(s,a') \right]\\
&=\eta\cdot\mathbb{E}_{a'\sim\pi^k(s)}\left[ (\mathbb{I}(a=a')-\pi^k(a\mid s)) \cdot A(s,a') \right]\\
&=\eta\cdot\sum_{a'\in\mathcal{A}}\left[\pi^k(a'\mid s) \cdot(\mathbb{I}(a=a')-\pi^k(a\mid s)) \cdot A(s,a') \right]\\
&=\eta\cdot\pi^k(a\mid s)\cdot\left[ (1-\pi^k(a\mid s))\cdot A(s,a)- \sum_{a'\neq a} \pi^k(a'\mid s)\cdot A(s,a') \right]\\
&=\eta\cdot\pi^k(a\mid s)\cdot\left[ A(s,a)- \sum_{a'\in\mathcal{A}} \pi^k(a'\mid s)\cdot A(s,a') \right]\\
\end{aligned}
\end{equation}
$$

其中，

$$
\begin{equation}
\begin{aligned}
\sum_{a\in\mathcal{A}} \pi(a\mid s)\cdot A(s,a)
&=\mathbb{E}_{a\sim\pi(s)}[A(s,a)]\\
&=\mathbb{E}_{a\sim\pi(s)}[Q(s,a)] - \mathbb{E}_{a\sim\pi(s)}[V(s)]\\
&=V(s)-V(s)\\
&=0
\end{aligned}
\end{equation}
$$

因此，

$$
\begin{equation}
\begin{aligned}
z^{k+1}_{s,a}-z^{k}_{s,a}
&=\eta\cdot\pi^k(a\mid s)\cdot A(s,a)
\end{aligned}
\end{equation}
$$

得证。
