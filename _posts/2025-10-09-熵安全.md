---
layout:     post
title:      强化学习中的熵 (2)
subtitle:   熵安全策略
date:       2025-10-09
author:     Zifeng Mai
header-img: img/enoshima.jpg
preview:    本文探讨了强化学习中的熵安全 (Policy Safety) 策略，介绍了在近期提出了双边熵安全策略。双边熵安全包括两个方面：1. 防止策略熵坍塌 (entropy collapse)；2. 防止策略熵爆炸 (entropy explosion)。这些策略在实际应用中都有很好的效果，能够有效防止策略熵的不稳定，使得我们的模型具有更强的探索性，同时也能够保持模型的收敛性。
catalog: true
tags:
    - Reinforcement Learning
    - Policy (Shannon) Entropy
    - Policy Safety
---

## 引言

在上一篇文章中，我们介绍了策略熵以及为什么强化学习对于策略熵的影响。在本文中，我们介绍一下现代强化学习算法中用到的熵安全策略。

这里的熵安全包含两个方面：防止熵坍塌、防止熵爆炸，即双边熵安全。

后续的内容按照文章发表的先后顺序来介绍，并在持续更新中。



## 2025.03～2025.05

### DAPO

> https://arxiv.org/abs/2503.14476

DAPO中提出了Clip Higher的策略，将PG损失函数中Clip的上限提高 (0.2 --> 0.28)：

$$
\begin{equation}
\mathcal{J}(\theta)=\mathbb{E}\left[\frac{1}{\sum_{i=1}^G|o_i|}\sum_{i=1}^G \sum_{t=1}^{|o_i|} \min(r_{i,t}(\theta) A_{i,t}, \text{clip}(r_{i,t}(\theta),1-\varepsilon_{low},1+\varepsilon_{high})) \right]
\end{equation}
$$

这使得有更多低概率token被采纳，而不是被clip导致没有梯度。

### Skywork-OR1

> https://arxiv.org/abs/2505.22312

在昆仑万维的Skywork-OR1中，使用了一个动态权重的熵损失函数。

在第 $k$ 次更新时，熵损失为：

$$
\begin{equation}
\begin{aligned}
\mathcal{L}_{ent}^k&=\alpha_ke_k\\
e_k&=\sum_{t=1}^{|y|}\sum_{v=1}^{|\mathcal{V}|}\pi_\theta(y_t^v)\log\pi_\theta(y_t^v)
\end{aligned}
\end{equation}
$$

其中，系数 $\alpha_k$ 是动态计算的：

$$
\begin{equation}
\begin{aligned}
\alpha_k&=c_k\cdot\mathbb{I}(e_k\le\mathcal{H}^*)\\
c_{k+1}&=
\begin{cases}
c_k+\Delta,&e_k<\mathcal{H}^*\\
c_k-\Delta,&e_k>\mathcal{H}^*\\
\end{cases}
\end{aligned}
\end{equation}
$$

这里有两个重要的超参数，$\Delta=5e-3$ 表示调整的步长，$\mathcal{H}^*=0.2$ 表示目标的策略熵。

### Seed-GRPO

> https://arxiv.org/abs/2505.12346
>
> https://zhuanlan.zhihu.com/p/1913635433750992154

这篇工作中将语义熵 (Semantic Entropy) 融入训练过程中，用于衡量模型对当前题目 (prompt) 的不确定性。

语义熵是指模型在输入同一个prompt 时所生成的多个答案之间语义上的多样性。比如说对于一个数学题，如果模型生成的多个答案在数字上可能不同，但逻辑一致、结论相同，那么语义熵是低的；反之，如果模型给出了互相矛盾的推理，那语义熵就高。

语义熵的计算方法是基于语义聚类的。首先将一个组的 $G$ 个response进行聚类，得到 $K$ 个类中心 $\{C_1,\cdots,C_K\}$，然后在簇的粒度下计算熵：

$$
\begin{equation}
\begin{aligned}
SE(q)&=-\frac{1}{K}\sum_{k=1}^K\log p(C_k\mid q)\\
p(C_k\mid q) &= \sum_{o_i\in C_k}\pi_{\theta_{old}}(o_i\mid q)
\end{aligned}
\end{equation}
$$

Seed-GRPO的主要思想：让模型根据语义熵动态调节训练强度（相当于一种课程学习）

- 当语义熵很大时，说明这个任务对于模型来说太难了，因此应该减小更新幅度
- 当语义熵比较小时，说明模型能够很好处理这个任务，可以加大训练的力度

$$
\begin{equation}
\hat{A_i}=A_i\cdot f\left(\alpha\cdot\frac{SE(q)}{SE_{max}(q)}\right)
\end{equation}
$$

> 本人在这里有个疑问。算法的核心假设在于：模型学会做简单题之后，做难题的能力也会有提升。
>
> 这样会不会容易过拟合到简单题上，难题一直没有有效的梯度？

### KL-Cov / Clip-Cov

> http://arxiv.org/abs/2505.22617

在上一篇文章中我们提到，策略熵的改变量是取决于token的【概率】和【优势】的协方差，其中：

- 高概率 + 高优势 token --> 熵减
- 低概率 + 高优势 token --> 熵增

因此，想要防止熵坍塌，一个很自然的方法就是惩罚那些【协方差较高】的token。

对于某个token $y_i$，其协方差的计算公式为：

$$
\begin{equation}
Cov(y_i)=\left( \log\pi_\theta(y_i)-\frac{1}{|y|}\sum_{j=1}^{|y|}\log\pi_\theta(y_j) \right)
\cdot
\left( A(y_i)-\frac{1}{|y|}\sum_{j=1}^{|y|}A(y_j) \right)
\end{equation}
$$

论文中提出了两种惩罚策略。

**KL-Cov.** 选择协方差最大的 $k$ 个token，并增加KL散度的惩罚：

$$
\begin{equation}
\mathcal{L}_{\text{KL-cov}}(\theta)=
r_t(\theta)A_t-\mathbb{I}(t\in I_{KL})\cdot\beta\cdot D_{KL}(\pi_{\theta_{old}}\|\pi_\theta)
\end{equation}
$$

**Clip-Cov.** 随机选择一定比例的高协方差token，将其梯度截断。



## 2025.06～2025.07

### 熵优势

> https://arxiv.org/abs/2506.14758

LLM的一些探索性推理行为往往伴随着一些高熵token的生成，如关键决策点、反思纠错、创新行为、aha moment等。

文章在优势函数中新增一个与策略熵有关的项，以此放大高熵区域的优化信号，鼓励模型生成更长、更深入的CoT。

首先计算第 $t$ 个token的熵：

$$
\begin{equation}
\mathcal{H}_t=-\sum_{v\in\mathcal{V}}\pi_\theta(v\mid q,o_{<t})\log\pi_\theta(v\mid q,o_{<t})
\end{equation}
$$

然后定义一个entropy-based优势项：

$$
\begin{equation}
\psi(\mathcal{H}_t)=\min\left( \alpha\cdot\mathcal{H}_t^{\text{detach}},\frac{|A_t|}{\kappa} \right)
\end{equation}
$$

其中，$\alpha$ 是缩放系数，$\kappa$ 是裁剪阈值。$\mathcal{H}_t^{\text{detach}}$ 表示不计算熵的梯度。

最终的优势函数为：

$$
\begin{equation}
A_{t}^{\text{shaped}}=A_t+\psi(\mathcal{H}_t)
\end{equation}
$$

这个优势函数的设计有两个好处：

1. 不直接优化熵，避免模型生成无意义的随机文本。
2. $\psi(\mathcal{H}_t)$ 被限制在原始优势值的一定比例内，确保熵优势项不会过度主导原始优势。如果某个动作本来是坏的 ($A_t<0$)，但熵非常大，这个截断机制仍然能保持这个动作的优势为负。



## 2025.08～2025.09

### SIREN

> https://arxiv.org/abs/2509.25133
>
> https://zhuanlan.zhihu.com/p/1957408206369305372

文章首先分析了普通的熵正则化失效的原因，具体有两个方面：

1. 从单步生成的角度：LLM的动作空间非常大，通常包含数十万个token。朴素熵正则化的目标是最大化策略熵，这意味着鼓励模型将所有token的概率摊平。

   假设原始模型99%的概率集中在10个有意义的token上。此时，只需从这10个token中匀出微不足道的1%的概率，并将其均匀地分配给剩下的无意义的token，就能带来巨大的熵增益。优化器会迅速发现这条增加熵的捷径，从而牺牲任务本身的奖励，导致模型为了**探索而探索**，最终输出无意义的内容。

   因此，有效的探索不应是在整个词表中漫无目的地游走，而应该被约束在一个有意义的、高质量的动作子集内，即那些最有可能引导出正确答案的token集合。

2. 从整条轨迹的角度：由于LLM是自回归生成的，因此单个token的不确定性会在整个轨迹中累积放大。由于朴素熵正则化对所有位置一视同仁地施加熵奖励，导致模型在每一个位置都试图最大化不确定性。这种全局性的高熵，使得整个CoT从第一步开始就充满了噪声，最终完全崩溃。

   因此，训练算法应该识别出轨迹中那些真正需要探索的关键决策点，并仅在这些位置施加正则化。

基于这两个分析，文章提出了以下的方法。

**利用Top-p掩码约束动作空间** 

策略核的定义为：满足累积概率大于 $p$ 的最小token集合。即：

$$
\begin{equation}
\begin{aligned}
&\mathcal{V}^{(p)}=\arg\min_{S\subseteq \mathcal{V}} |S|\\
&\text{s.t.} \sum_{v\in S}\pi_\theta(v\mid v)\ge p
\end{aligned}
\end{equation}
$$

我们仅在策略核上计算熵，得到每个token的熵 $\mathcal{H}_j'$。

**利用熵峰值掩码识别关键token**

对于一条完整轨迹 $o_i$，首先计算出每个token的熵，然后计算一个熵的分位阈值：

$$
\begin{equation}
q^{(i)}=\text{Quantile}_{\tau}\{\mathcal{H}_1',\mathcal{H}_2',\dots,\mathcal{H}_{|o_i|}'\}
\end{equation}
$$

构造如下掩码：

$$
\begin{equation}
m_j=
\begin{cases}
1,&\mathcal{H}_j'\ge q^{(i)}\\
0,&otherwise
\end{cases}
\end{equation}
$$

通过这个掩码，我们就能找到高熵token，能够精准地在这些关键token上施加熵正则化：

$$
\begin{equation}
\hat{\mathcal{H}}=\frac{1}{\sum_j\mathbb{I}(m_j=1)}\sum_{v_j\in o_i}\mathbb{I}(m_j=1)\cdot\mathcal{H}_j'
\end{equation}
$$

熵正则化损失定义如下：

$$
\begin{equation}
\mathcal{L}_{ent}=(\hat{\mathcal{H}}-\mathcal{H}^*)^2
\end{equation}
$$

其中，$\mathcal{H}^*$ 是训练第一个batch计算得到的熵，不需要引入超参数。

### QAE

> https://arxiv.org/abs/2509.22611

当前的value-free RL方法（如GRPO、DAPO）容易出现熵坍塌或熵爆炸的核心原因在于，这些方法普遍采用组内均值作为baseline来估计优势函数。这种方法会惩罚那些有价值的、但是奖励为负的动作轨迹，导致训练不稳定。

文章首先通过实验得到了以下四个观察：

1. DAPO中的clip higher机制能够在训练初期能够极大提升一些关键token的概率，但很快就收敛，因此不能保证稳定的推理能力增强。
2. Clip higher方法会导致模型生成同质化的、低质量的探索轨迹。
3. 熵爆炸的现象主要是由于负优势的样本。
4. DAPO对于超参数 ($\varepsilon_{high}$) 的设置非常敏感。

文章提出使用K分位数代替均值来作为baseline。

具体来说，给定一个query $q$，我们利用 $\pi_{old}$ 采样 $G$ 个reponse $\{o_i\}$，并为每个response打分得到 $G$ 个二元奖励 $R_i\in\{0,1\}$。

我们可以定义如下的经验成功率和累积分布函数CDF：

$$
\begin{equation}
\begin{aligned}
p(q)&:=\frac{1}{G}\sum_{i=1}^GR_i\\
\hat{F}_q(x)&:=\frac{1}{G}\sum_{i=1}^G\mathbb{I}(R_i\lt x)
\end{aligned}
\end{equation}
$$

接着，我们就能够定义K分位数baseline：

$$
\begin{equation}
\begin{aligned}
b_K(q)&:=\inf\{ x:\hat{F}_q(x)\ge K \}\\
&=
\begin{cases}
0,&p(q)\le1-K\\
1,&p(q)>1-K
\end{cases}
\end{aligned}
\end{equation}
$$

其中，$K\in(0,1)$。

从K分位数的定义我们可以发现，K分位数实际上是根据难度阈值 $1-K$ 来划分：

- 对于难样本，即成功率小于 $1-K$ 的样本，baseline为0。此时错误response的优势为0，罕见的正确response得到正优势1，增强了生成这些正确response的概率。
- 对于简单样本，即成功率大于 $1-K$ 的样本，baseline为1。此时会降低生成错误response的概率。

**熵安全理论分析**

QAE可以从理论上证明其双边熵安全性，即可以同时防止熵爆炸和熵坍塌。下面的定理说明了这一点

**Theorem 1 (Two-regime Entropy safty for K-quantile).** 对于给定的 $q$ 以及如上定义的K分位数 $b_K(q)$，我们有：

- （防止难样本导致熵爆炸）如果 $p(q)\le 1-K$，对于任意的baseline $b\in[0,1]$，我们有：

  $$
  \begin{equation}
  \Delta\mathcal{H}(q;b_K)\le \Delta\mathcal{H}(q;b)
  \end{equation}
  $$

- （防止简单样本导致熵坍塌）如果 $p(q)> 1-K$，对于任意的baseline $b\in[0,1]$，我们有：

  $$
  \begin{equation}
  \Delta\mathcal{H}(q;b_K)\ge \Delta\mathcal{H}(q;b)
  \end{equation}
  $$

其中，$\Delta\mathcal{H}(q;b)$ 表示以 $b$ 为基线，使用PG算法对softmax策略进行一步梯度更新之后策略熵的变化量。

由上一篇文章我们可以知道，这个变化量可以由下面的等式定量描述：

$$
\begin{equation}
\Delta\mathcal{H}(q;d)\approx -\eta\cdot\text{Cov}_{y\sim\pi(q)}\left( \log\pi(y\mid q),\pi(y\mid q)A_b(y,q) \right)
\end{equation}
$$

定理1说明了，所有baseline中，使用 $b_K(q)$ 无论对于难样本还是简单样本都会使得策略熵的变化量最大或最小，从而保证了双边熵安全。
