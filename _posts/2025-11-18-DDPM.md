---
layout:     post
title:      生成模型 (1.3)
subtitle:   Denoising Diffusion Probabilistic Model
date:       2025-11-18
author:     Zifeng Mai
header-img: img/arashiyama.jpg
preview:    本文深入介绍了DDPM的数学原理，包括DDPM的前向过程和反向过程、条件化技巧、ELBO的推导以及优化、DDPM的采样过程。通过本文，读者能够全面了解DDPM的工作原理，以及如何应用DDPM来解决生成模型的问题。
catalog: true
tags:
    - Generative Modeling
    - Denoising Diffusion Probabilistic Model
---

## 引言

DDPMs (Denoising Diffusion Probabilistic Models) 是扩散生成模型的基石。和VAE与HVAE类似，DDPM在变分框架下解决复杂分布的建模问题，但DDPM通过一些巧妙的技巧，解决了VAE和HVAE所面临的一些问题。

DDPM的核心是下面两个随机过程：

1. **前向过程**（固定的编码器）：在前向过程中，DDPM通过向数据中逐步注入高斯噪声 $p(x_i\mid x_{i-1})$ 来逐步地破坏数据。随着加噪步数趋于无穷，原始数据就会被破坏为纯高斯噪声。因此DDPM的前向过程是固定的，没有可学习的参数。
2. **反向过程**（可学习的解码器）：在反向过程中，DDPM使用神经网络 $p_\phi(x_{i-1}\mid x_i)$ 来学习如何从被破坏的纯噪声中还原得到原始数据。每一个去噪步骤都和VAE类似，希望从一个隐变量中还原原始的数据。

和VAE不同的是，DDPM使用固定的编码器，专注于提升解码器的去噪能力，因而展示出非常强大的生成能力和稳定性。

## 一、DDPM的数学模型

### 1.1. 前向过程

在DDPM中，前向过程使用一个固定的、不可学习的编码器来逐步破坏原始数据，直至其变为纯高斯噪声 $p_{prior}=\mathcal{N}(0,1)$。

前向过程的每一步可以通过下面的高斯转移核 (Gaussian transition kernel) 来描述：

$$
\begin{equation}
p(x_i\mid x_{i-1})=\mathcal{N}(x_i;\sqrt{1-\beta_i^2}x_{i-1},\beta_i^2I)
\end{equation}
$$

其中，$x_0\sim p_{data}$ 是从真实数据分布中采样得到的样本。

超参数 $\beta_i\in(0,1)$ 表示第 $i$ 步所加高斯噪声的方差，且满足 $\beta_{i}\gt\beta_{i-1}$。为了表示方便，我们记 $\alpha_i=\sqrt{1-\beta_i^2}$，则高斯转移核等价于下面的迭代方程：

$$
\begin{equation}
x_i=\alpha_i x_{i-1}+\beta_i\epsilon_i
\end{equation}
$$

其中，$\epsilon_i\sim\mathcal{N}(0,1)$ 是来自标准高斯分布的独立同分布噪声。

由公式 $(1)$，我们可以写出在任意时间步 $i$ 时，数据点 $x_i$ 的概率分布（证明见附录）：

$$
\begin{equation}
p_i(x_i\mid x_0)=\mathcal{N}(x_i;\overline{\alpha}_ix_0,\left(1-\overline{\alpha}_i^2\right)I)
\end{equation}
$$

其中，

$$
\begin{equation}
\overline{\alpha}_i:=\prod_{k=1}^i\sqrt{1-\beta_k^2}=\prod_{k=1}^i\alpha_k
\end{equation}
$$

同样地，我们也可以用递推式来表示：

$$
\begin{equation}
x_i=\overline{\alpha}_i x_0+\sqrt{1-\overline{\alpha}_i^2}\epsilon
\end{equation}
$$

由于 $\{\beta_i\}$ 是一个递增序列，因此 $\lim_{i\rightarrow\infty}\beta_i=1$，即 $\lim_{i\rightarrow\infty}\alpha_i=\alpha_i=\sqrt{1-\left(\lim_{i\rightarrow\infty}\beta_i\right)^2}=0$，因此 $\lim_{i\rightarrow\infty}\overline{\alpha}_i=0$。

也就是说，当加噪步数 $L\rightarrow\infty$ 时，最终数据点会被破坏为纯高斯噪声：

$$
\begin{equation}
\lim_{L\rightarrow\infty} p(x_L\mid x_0)\rightarrow p_{prior}:=\mathcal{N}(0,1)
\end{equation}
$$

此时破坏后的数据 $x_L$ 的分布与原始数据 $x_0$ 无关。

### 1.2. 反向去噪过程

DDPM的核心之处在于其反向去噪过程。在反向过程中，DDPM可以逐步地逆转前向过程中被破坏的数据。

反向过程是一个马尔可夫链，从纯高斯噪声 $x_L\sim p_{prior}$ 开始，目标是逐步将其中的噪声进行去除，直到产生一个服从原始数据分布的数据 $x_0\sim p_{data}$。

因此，DDPM中最为关键的问题在于：我们如何能够精确地计算（至少是高效地近似）反向转移核 $p(x_{i-1}\mid x_i)$。这个问题的难点在于，去噪过程中每一个时间步上数据的概率分布 $p(x_i)$ 都极其复杂，因此我们很难利用贝叶斯定理等方式来快速得到这个反向转移核。

从这里可以看出DDPM和变分推断的相似性所在：这两者都是希望估计一个难解的复杂的后验概率。因此，在DDPM中也同样引入ELBO作为优化目标。在DDPM的原论文中是直接从ELBO的定义开始推导，在本文中，我们试图使用一种更加易于理解的方式：利用条件概率来得到一个可解的表达式。

#### 1.2.1. 训练目标

我们的目标是对难解的反向转移核 $p(x_{i-1}\mid x_i)$ 进行精确的估计。为此，我们引入一个可解的变分分布 $p_{\phi}(x_{i-1}\mid x_i)$，通过训练参数 $\phi$ 来找到最优的近似，即我们希望找到如下的最优参数 $\phi^*$：

$$
\begin{equation}
\phi^*=\arg\min_{\phi}\mathbb{E}_{p_i(x_i)}[\mathcal{D}_{KL}(p(x_{i-1}\mid x_i)\|p_{\phi}(x_{i-1}\mid x_i))]
\end{equation}
$$

通过贝叶斯定理，我们可以写出后验分布 $p(x_{i-1}\mid x_i)$ 的形式：

$$
\begin{equation}
p(x_{i-1}\mid x_i)=p(x_i\mid x_{i-1})\frac{p_{i-1}(x_{i-1})}{p_i(x_i)}
\end{equation}
$$

其中，$p_{i-1}(x_{i-1})$ 和 $p_{i}(x_{i})$ 分别表示在第 $i-1$ 步和第 $i$ 步中数据的边缘分布，其表达式如下：

$$
\begin{equation}
p_i(x_i)=\int p_i(x_i\mid x_0)p_{data}(x_0)\mathrm{d}x_0
\end{equation}
$$

然而，由于原始数据分布 $p_{data}$ 是未知的，因此这个积分不存在闭式解，即后验分布 $p(x_{i-1}\mid x_i)$ 是难解的。

#### 1.2.2. 条件化技巧

DDPM的一个核心insight在于：我们在给定某个干净数据样本 $x$ 的情况下对后验概率进行近似。这种“条件化”技巧虽然简单，但非常实用，是一种使得难解分布变得可解的常用技巧（我们在流模型中可以看到，通过 Conditioning Flows 来避免 Normalizing Flows 中对ODE的模拟）：

$$
\begin{equation}
p(x_{i-1}\mid x_i,x)=p(x_i\mid x_{i-1})\frac{p_{i-1}(x_{i-1}\mid x)}{p_i(x_i\mid x)}
\end{equation}
$$

为什么通过引入初始条件之后，就可以使得后验概率变得可解？这来自于前向过程中两个非常重要的性质：

1. 前向过程具有**马尔可夫性**，即 $p(x_i\mid x_{i-1},x)=p(x_i\mid x_{i-1})$；
2. 前向过程具有**高斯性**：即 $p(x_i\mid x_{i-1})$ 服从高斯分布。

我们即将看到 (Lemma 1)，这两个性质使得条件后验概率 $p(x_{i-1}\mid x_i,x)$ 也会服从高斯分布，因此其具有闭式解，是可解的。

更重要的是，这种巧妙的条件化技巧使得我们可以推导出在功能上和公式 $(7)$ 等价的一个可解的优化目标。下面的 Theorem 1 说明了这一点。

**Theorem 1 (优化边缘KL和条件KL的等价性).** 我们有如下的等式成立：

$$
\begin{equation}
\begin{aligned}
&\mathbb{E}_{p_i(x_i)}[\mathcal{D}_{KL}(p(x_{i-1}\mid x_i)\|p_{\phi}(x_{i-1}\mid x_i))]\\
=&\mathbb{E}_{p_{data}(x)}\mathbb{E}_{p(x_i\mid x)}[\mathcal{D}_{KL}(p(x_{i-1}\mid x_i,x)\|p_{\phi}(x_{i-1}\mid x_i))]+C
\end{aligned}
\end{equation}
$$

其中，$C$ 是一个与参数 $\phi$ 无关的常数。

更进一步，使得上式最小的变分分布满足：

$$
\begin{equation}
\begin{aligned}
p^*(x_{i-1}\mid x_i)
&=\mathbb{E}_{p(x_i\mid x)}[p(x_{i-1}\mid x_i,x)]\\
&=p(x_{i-1}\mid x_i)
\end{aligned}
\end{equation}
$$

定理1的完整证明在附录中。



定理1揭示了一个非常重要的等价性质：最小化两个边缘分布之间的KL散度，在数学上等价于最小化某些条件分布之间的KL散度。当我们引入条件分布之后，$p(x_{i-1}\mid x_i,x)$ 是一个高斯分布，就变得可解了。下面的引理说明了这一点。

**Lemma 1 (反向条件转移核).** $p(x_{i-1}\mid x_i,x)$ 是一个高斯分布，其形式如下：

$$
\begin{equation}
p(x_{i-1}\mid x_i,x)=\mathcal{N}\left( x_{i-1}\mid \mu(x_i,x,i),\sigma^2(i)I \right)
\end{equation}
$$

其中，

$$
\begin{equation}
\begin{aligned}
\mu(x_i,x,i)&:=\frac{\overline{\alpha}_{i-1}\beta_i^2}{1-\overline{\alpha}_{i}^2}x + \frac{\left(1-\overline{\alpha}_{i-1}^2\right)\alpha_i}{1-\overline{\alpha}_{i}^2}x_{i}\\
\sigma^2(i)&:=\frac{1-\overline{\alpha}_{i-1}^2}{1-\overline{\alpha}_{i}^2}\beta_i^2
\end{aligned}
\end{equation}
$$

引理1的完整证明在附录中。

## 二、建模反向转移核

定理1为我们揭示了DDPM的一个核心insight：**边缘KL散度和条件KL散度在梯度上是等价的**。

由于在引理1中，我们已经知道 $p(x_{i-1}\mid x_i,x)$ 是一个高斯分布，因此DDPM假设反向转移核 $p_{\phi}(x_{i-1}\mid x_i)$ 也是是一个高斯分布：

$$
\begin{equation}
p_{\phi}(x_{i-1}\mid x_i)=\mathcal{N}\left( x_{i-1}\mid \mu_{\phi}(x_i,i),\sigma^2(i)I \right)
\end{equation}
$$

其中，$\mu_{\phi}(x_i,i)$ 是可学习的均值，而 $\sigma^2(i)$ 则是固定的常数。

因此，我们考虑优化每个时间步上的条件KL散度：

$$
\begin{equation}
\mathcal{L}_{\text{diffusion}}(x_0;\phi)=\sum_{i=1}^L \mathbb{E}_{p(x_i\mid x_0)}[\mathcal{D}_{KL}(p(x_{i-1}\mid x_i,x_0)\|p_{\phi}(x_{i-1}\mid x_i))]
\end{equation}
$$

其中，$x_0\sim p_{data}$。

在实际实现中，这个损失函数有多种实现方式：

### 2.1. mean prediction

由于所有分布都是高斯分布，因此公式 $(16)$ 可以化简为：

$$
\begin{equation}
\mathcal{L}_{\text{diffusion}}(x_0;\phi)=\sum_{i=1}^L \frac{1}{2\sigma^2(i)}\| \mu_\phi(x_i,i)-\mu(x_i,x_0,i) \|_2^2+C
\end{equation}
$$

我们对 $x_0$ 取期望，并忽略与 $\phi$ 无关的常数 $C$ 之后，就能够得到DDPM的训练目标：

$$
\begin{equation}
\mathcal{L}_{DDPM}(\phi)=\sum_{i=1}^L \frac{1}{2\sigma^2(i)} \mathbb{E}_{x_0\sim p_{data}}\mathbb{E}_{x_i\sim p(x_i\mid x_0)} \left[\| \mu_\phi(x_i,i)-\mu(x_i,x_0,i) \|_2^2\right]
\end{equation}
$$

### 2.2. $\epsilon$-prediction (noise prediction)

在实践中，一般不使用上面的mean prediction实现，而是会预测噪声 (noise prediction)。这两种方式是完全等价的，下面我们展示这一点。

前面我们提到，任何一个噪声样本 $x_i\sim p(x_i\mid x_0)$ 都可以由下面的表达式来生成：

$$
\begin{equation}
\begin{aligned}
x_i&=\overline{\alpha}_i x_0+\sqrt{1-\overline{\alpha}_i^2}\epsilon\\
x_0&\sim p_{data},\epsilon\sim\mathcal{N}(0,1)
\end{aligned}
\end{equation}
$$

代入公式 $(14)$ 可以得到，均值的表达式可以写为如下形式（证明见附录）：

$$
\begin{equation}
\begin{aligned}
\mu(x_i,x_0,i)
&:=\frac{\overline{\alpha}_{i-1}\beta_i^2}{1-\overline{\alpha}_{i}^2}x_0 + \frac{\left(1-\overline{\alpha}_{i-1}^2\right)\alpha_i}{1-\overline{\alpha}_{i}^2}x_{i}\\
&=\frac{1}{\alpha_i}\left( x_i-\frac{1-\alpha_i^2}{\sqrt{1-\overline{\alpha}_i^2}}\epsilon \right)
\end{aligned}
\end{equation}
$$

因此，预测均值 $\mu_\phi(x_i,i)$ 和预测噪声 $\epsilon_\phi(x_i,i)$ 是完全等价的：

$$
\begin{equation}
\begin{aligned}
\mu_\phi(x_i,i)
&=\frac{1}{\alpha_i}\left( x_i-\frac{1-\alpha_i^2}{\sqrt{1-\overline{\alpha}_i^2}}\epsilon_\phi(x_i,i) \right)
\end{aligned}
\end{equation}
$$

又由于均值和方差之间是线性关系，因此在训练目标中，二者可以完全等价替代，即：

$$
\begin{equation}
\| \mu_\phi(x_i,i)-\mu(x_i,x_0,i) \|_2^2\propto \| \epsilon_\phi(x_i,i)-\epsilon \|_2^2
\end{equation}
$$

二者只相差了与 $i$ 相关的一个常数。

在实际实现中，我们一般使用下面的loss来训练DDPM模型，这是最常用的训练方式：

$$
\begin{equation}
\begin{aligned}
\mathcal{L}_{DDPM}(\phi)
&=\mathbb{E}_{i}\mathbb{E}_{x_0\sim p_{data}}\mathbb{E}_{\epsilon\sim \mathcal{N}(0,1)} \left[\left\| \epsilon_\phi(x_i,i)-\epsilon \right\|_2^2\right]\\
&=\mathbb{E}_{i}\mathbb{E}_{x_0\sim p_{data}}\mathbb{E}_{\epsilon\sim \mathcal{N}(0,1)} \left[\left\| \epsilon_\phi(\overline{\alpha}_i x_0+\sqrt{1-\overline{\alpha}_i^2}\epsilon,i)-\epsilon \right\|_2^2\right]\\
\end{aligned}
\end{equation}
$$

### 2.3. $x$-prediction (clean prediction)

公式 $(14)$ 同样描述了均值 $\mu(x_i,x_0,i)$ 与干净样本 $x_0$ 之间的关系，因此还有一种等价的优化方式，称为 $x$-prediction，即训练一个神经网络 $x_\phi(x_i,i)$ 从噪声样本 $x_i$ 中预测干净样本 $x_0$：

$$
\begin{equation}
\begin{aligned}
\mu_\phi(x_i,i)
:=\frac{\overline{\alpha}_{i-1}\beta_i^2}{1-\overline{\alpha}_{i}^2}x_\phi(x_i,i) + \frac{\left(1-\overline{\alpha}_{i-1}^2\right)\alpha_i}{1-\overline{\alpha}_{i}^2}x_{i}\\
\end{aligned}
\end{equation}
$$

和上面的过程类似，由于均值和数据样本是线性关系，二者可以完全等价替代：

$$
\begin{equation}
\| \mu_\phi(x_i,i)-\mu(x_i,x_0,i) \|_2^2\propto \| x_\phi(x_i,i)-x_0 \|_2^2
\end{equation}
$$

这也就引出了在预测干净样本时使用的损失函数：

$$
\begin{equation}
\begin{aligned}
\mathcal{L}_{DDPM}(\phi)
&=\mathbb{E}_{i}\mathbb{E}_{x_0\sim p_{data}}\mathbb{E}_{\epsilon\sim \mathcal{N}(0,1)} \left[\omega_i\left\| x_\phi(x_i,i)-x_0 \right\|_2^2\right]\\
\end{aligned}
\end{equation}
$$

需要补充的是，$\epsilon$-prediction和 $x$-prediction是完全等价的，因为：

$$
\begin{equation}
\begin{aligned}
x_i&=\overline{\alpha}_i x_\phi(x_i,i)+\sqrt{1-\overline{\alpha}_i^2}\epsilon_\phi(x_i,i)
\end{aligned}
\end{equation}
$$

## 三、DDPM中的ELBO

在公式 $(15)$ 中，我们定义了如下的反向转移核：

$$
\begin{equation}
p_{\phi}(x_{i-1}\mid x_i)=\mathcal{N}\left( x_{i-1}\mid \mu_{\phi}(x_i,i),\sigma^2(i)I \right)
\end{equation}
$$

其中，$\mu_{\phi}(x_i,i)$ 是可学习的均值，而 $\sigma^2(i)$ 则是固定的常数。

因此，我们可以写出DDPM模型的联合分布以及边缘分布：

$$
\begin{equation}
\begin{aligned}
p_\phi(x_0,x_{1:L})
&:=p_{\phi}(x_0\mid x_1)p_{\phi}(x_1\mid x_2)\cdots p_{\phi}(x_{L-1}\mid x_L)p_{prior}(x_L)\\
p_\phi(x_0)
&:=\int p_\phi(x_0,x_{1:L})\mathrm{d}x_{1:L}
\end{aligned}
\end{equation}
$$

借此，我们就能够和VAE或HVAE那样，写出DDPM中ELBO的形式。下面的定理2说明了这一点：

**Theorem 2 (DDPM's ELBO).** 我们可以如下定义DDPM的ELBO项，这也是对数概率密度的下界：

$$
\begin{equation}
\begin{aligned}
-\log p_\phi(x_0)
&\le -\mathcal{L}_{ELBO}(x_0;\phi)\\
&:= \mathcal{L}_\text{prior}(x_0)+\mathcal{L}_\text{recon}(x_0;\phi)+\mathcal{L}_\text{diffusion}(x_0;\phi)
\end{aligned}
\end{equation}
$$

其中，

$$
\begin{equation}
\begin{aligned}
\mathcal{L}_\text{prior}(x_0)
&:=\mathcal{D}_{KL}\left( p(x_L\mid x_0)\| p_{prior}(x_L) \right)
\\
\mathcal{L}_\text{recon}(x_0;\phi)
&:=\mathbb{E}_{p(x_1\mid x_0)}\left[ -\log p_{\phi}(x_0\mid x_1) \right]
\\
\mathcal{L}_\text{diffusion}(x_0;\phi)
&:=\sum_{i=1}^L \mathbb{E}_{p(x_i\mid x_0)}[\mathcal{D}_{KL}(p(x_{i-1}\mid x_i,x_0)\|p_{\phi}(x_{i-1}\mid x_i))]
\end{aligned}
\end{equation}
$$

定理2的证明在附录中。

ELBO中包括三项：

- $\mathcal{L}_{\text{prior}}$ 用于保证前向过程能收敛到先验分布 $p_{prior}$ 上。只要我们选择合适的 $\{\beta_{i}\}$，使得 $\alpha_{L}$ 足够小，此时有 $p(\cdot\mid x_{0})\approx p_{prior}(\cdot)$ 即可。在实践中我们一般有下面几种选择方式：
  1. 线性调度：$\beta_{i}=\beta_{start}+\frac{i}{L}(\beta_{end}-\beta_{start})$；
  2. 余弦调度：$\overline{\alpha}_{i}=\cos^2\left( \frac{\pi}{2}\cdot\frac{i}{L} \right)$。
- $\mathcal{L}_{\text{recon}}$ 用于保证第一步的重建精度，我们一般通过蒙特卡洛估计来近似和优化这一项。在实践中，我们一般把这一步融合进入 $t=2,\dots,L$ 的噪声估计loss中来优化。
- $\mathcal{L}_{\text{diffusion}}$ 用于拉近变分分布 $p_{\phi}(x_{i-1}\mid x_{i})$ 和真实反向转移核 $p(x_{i-1}\mid x_{i})$ 之间的距离。正如我们之前介绍的，共有3中不同的实现方式，其中最常用的是预测噪声。



## 四、DDPM的采样

我们假设使用 $\epsilon$-prediction 的方式进行训练，在训练完成后，我们得到了一个噪声预测模型 $\epsilon_\phi^*(x_i,i)$。

在采样过程中我们从一个纯噪声 $x_L\sim\mathcal{N}(0,1)$ 开始，逐步使用噪声预测模型进行采样，直到得到干净样本 $x_0$：

$$
\begin{equation}
\begin{aligned}
x_{i-1}
&=\frac{1}{\alpha_i}\left( x_i-\frac{1-\alpha_i^2}{\sqrt{1-\overline{\alpha}_i^2}}\epsilon_\phi^*(x_i,i) \right)+\sigma(i)\epsilon_i
\end{aligned}
\end{equation}
$$

其中，$\epsilon_i\sim\mathcal{N}(0,1)$。



## 附录

### 公式 (3) 的证明

下面我们证明在DDPM的前向过程中，如果使用如下的高斯转移核：

$$
\begin{equation}
p(x_i\mid x_{i-1})=\mathcal{N}(x_i;\sqrt{1-\beta_i^2}x_{i-1},\beta_i^2I)
\end{equation}
$$

则在任意时间步 $i$ 时，数据点 $x_i$ 的概率分布为

$$
\begin{equation}
p(x_i\mid x_0)=\mathcal{N}(x_i;\overline{\alpha}_ix_0,\left(1-\overline{\alpha}_i^2\right)I)
\end{equation}
$$

其中，

$$
\begin{equation}
\begin{aligned}
\overline{\alpha}_i&:=\prod_{k=1}^i\sqrt{1-\beta_k^2}=\prod_{k=1}^i\alpha_k\\
\alpha_i&=\sqrt{1-\beta_i^2}
\end{aligned}
\end{equation}
$$

我们使用数学归纳法。

**归纳基**：当 $i=1$ 时，有 $\overline{\alpha}_1=\alpha_1=\sqrt{1-\beta_1^2}$，且 $\left(1-\overline{\alpha}_1^2\right)=\left(1-\alpha_1^2\right)=\beta_1^2$。

代入公式 $(1)$ 得：

$$
\begin{equation}
\begin{aligned}
p(x_1\mid x_0)
&=\mathcal{N}(x_1;\sqrt{1-\beta_1^2}x_0,\beta_1^2I)\\
&=\mathcal{N}(x_1;\overline{\alpha}_1x_0,\left(1-\overline{\alpha}_1^2\right)I)
\end{aligned}
\end{equation}
$$

**归纳步**：假设当 $i=n$ 时，公式 $(3)$ 成立。

当 $i=n+1$ 时，考虑从 $x_n$ 到 $x_{n+1}$ 的转移，我们有：

$$
\begin{equation}
x_{n+1}=\alpha_{n+1} x_n+\beta_{n+1}\epsilon_{n+1}
\end{equation}
$$

代入归纳假设，得：

$$
\begin{equation}
\begin{aligned}
x_{n+1}
&=\alpha_{n+1} x_n+\beta_{n+1}\epsilon_{n+1}\\
&=\alpha_{n+1} \left( \overline{\alpha}_n x_0+\sqrt{1-\overline{\alpha}_n^2}\epsilon_n \right)+\beta_{n+1}\epsilon_{n+1}\\
&=\alpha_{n+1}\overline{\alpha}_n x_0 + \alpha_{n+1}\sqrt{1-\overline{\alpha}_n^2}\epsilon_n+\beta_{n+1}\epsilon_{n+1}\\
&=\overline{\alpha}_{n+1} x_0 + \alpha_{n+1}\sqrt{1-\overline{\alpha}_n^2} \epsilon_n + \beta_{n+1} \epsilon_{n+1}\\
\end{aligned}
\end{equation}
$$

我们计算 $x_{n+1}$ 的均值和方差。由于 $\epsilon_n$ 和 $\epsilon_{n+1}$ 都是服从 $\mathcal{N}(0,1)$ 的独立同分布随机向量，因此：

$$
\begin{equation}
\begin{aligned}
\mathbb{E}[x_{n+1}\mid x_0]
&=\overline{\alpha}_{n+1}\\

\text{Var}(x_{n+1}\mid x_0)
&=\left( \alpha_{n+1}\sqrt{1-\overline{\alpha}_n^2}\right)^2 + \beta_{n+1}^2\\
&=\alpha_{n+1}^2\left(1-\overline{\alpha}_n^2\right)+ \beta_{n+1}^2\\
&=\left(1-\beta_{n+1}^2\right)\left(1-\overline{\alpha}_n^2\right)+ \beta_{n+1}^2\\
&=1-\overline{\alpha}_n^2-\beta_{n+1}^2+\overline{\alpha}_n^2\beta_{n+1}^2+ \beta_{n+1}^2\\\\
&=1-\overline{\alpha}_n^2\left( 1- \beta_{n+1}^2\right)\\
&=1-\left( \overline{\alpha}_n\alpha_{n+1} \right)^2\\
&=1-\overline{\alpha}_{n+1}^2
\end{aligned}
\end{equation}
$$

由于对高斯分布进行线性变换不改变其高斯性，因此 $p(x_{n+1}\mid x_0)$ 仍然服从高斯分布，即：

$$
\begin{equation}
p(x_{n+1}\mid x_0)=\mathcal{N}(x_{n+1};\overline{\alpha}_{n+1}x_0,\left(1-\overline{\alpha}_{n+1}^2\right)I)
\end{equation}
$$

得证。

### Proof on Theorem 1.

定理1描述了最小化边缘KL和条件KL的等价性，下面我们证明这一点。

首先，我们展开等式的右侧：

$$
\begin{equation}
\begin{aligned}
\text{RHS}
&=\mathbb{E}_{p_{data}(x)}\mathbb{E}_{p(x_i\mid x)}[\mathcal{D}_{KL}(p(x_{i-1}\mid x_i,x)\|p_{\phi}(x_{i-1}\mid x_i))]\\
&=\mathbb{E}_{p(x_i, x)}[\mathcal{D}_{KL}(p(x_{i-1}\mid x_i,x)\|p_{\phi}(x_{i-1}\mid x_i))]\\
&=\int\int p(x_i, x)\mathcal{D}_{KL}(p(x_{i-1}\mid x_i,x)\|p_{\phi}(x_{i-1}\mid x_i))\mathrm{d}x\mathrm{d}x_i
\end{aligned}
\end{equation}
$$

根据KL散度的定义：

$$
\begin{equation}
\begin{aligned}
\mathcal{D}_{KL}(p(x_{i-1}\mid x_i,x)\|p_{\phi}(x_{i-1}\mid x_i))
&=\int p(x_{i-1}\mid x_i,x)\log\frac{p(x_{i-1}\mid x_i,x)}{p_{\phi}(x_{i-1}\mid x_i)}\mathrm{d}x_{i-1}
\end{aligned}
\end{equation}
$$

代入上式得：

$$
\begin{equation}
\begin{aligned}
\text{RHS}
&=\int\int\int p(x_i, x)p(x_{i-1}\mid x_i,x)\log\frac{p(x_{i-1}\mid x_i,x)}{p_{\phi}(x_{i-1}\mid x_i)} \mathrm{d}x_{i-1}\mathrm{d}x\mathrm{d}x_i\\
&=\int p(x_i)\int p(x\mid x_i) \int p(x_{i-1}\mid x_i,x)\log\frac{p(x_{i-1}\mid x_i,x)}{p_{\phi}(x_{i-1}\mid x_i)} \mathrm{d}x_{i-1}\mathrm{d}x\mathrm{d}x_i\\
&=\mathbb{E}_{p(x_i)}\left[ \mathbb{E}_{p(x\mid x_i)}\left[ \mathbb{E}_{p(x_{i-1}\mid x_i,x)}\left[ \log\frac{p(x_{i-1}\mid x_i,x)}{p_{\phi}(x_{i-1}\mid x_i)} \right] \right] \right]\\
&=\mathbb{E}_{p(x_i)}\left[ \mathbb{E}_{p(x\mid x_i)}\left[ \mathbb{E}_{p(x_{i-1}\mid x_i,x)}\left[ \log\frac{p(x_{i-1}\mid x_i,x)}{p(x_{i-1}\mid x_i)} \right] \right] \right]+\mathbb{E}_{p(x_i)}\left[ \mathbb{E}_{p(x\mid x_i)}\left[ \mathbb{E}_{p(x_{i-1}\mid x_i,x)}\left[ \log\frac{p(x_{i-1}\mid x_i)}{p_{\phi}(x_{i-1}\mid x_i)} \right] \right] \right]\\
&=\mathbb{E}_{p(x_i)}\left[ \mathbb{E}_{p(x\mid x_i)}\left[\mathcal{D}_{KL}(p(x_{i-1}\mid x_i,x)\|p(x_{i-1}\mid x_i))\right] \right]+\mathbb{E}_{p(x_i)}\left[ \mathcal{D}_{KL}(p(x_{i-1}\mid x_i))\|p_{\phi}(x_{i-1}\mid x_i))\right]\\
&=C+\text{LHS}
\end{aligned}
\end{equation}
$$

其中，第一项是和参数 $\phi$ 无关的常数。

得证。



### Proof on Lemma 1.

我们先重新说明一下符号定义。在DDPM的前向过程中，我们定义了如下的前向转移核：

$$
\begin{equation}
p(x_i\mid x_{i-1})=\mathcal{N}(x_i;\sqrt{1-\beta_i^2}x_{i-1},\beta_i^2I)
\end{equation}
$$

我们记 $\alpha_i=\sqrt{1-\beta_i^2}$，则在任意时间步 $i$ 时，数据点 $x_i$ 的概率分布为：

$$
\begin{equation}
p(x_i\mid x_0)=\mathcal{N}(x_i;\overline{\alpha}_ix_0,\left(1-\overline{\alpha}_i^2\right)I)
\end{equation}
$$

其中，

$$
\begin{equation}
\overline{\alpha}_i:=\prod_{k=1}^i\sqrt{1-\beta_k^2}=\prod_{k=1}^i\alpha_k
\end{equation}
$$

引理1描述了条件后验分布 $p(x_{i-1}\mid x_i,x_0)$ 是一个高斯分布，其均值和方差分别为：

$$
\begin{equation}
\begin{aligned}
\mu(x_i,x_0,i)&:=\frac{\overline{\alpha}_{i-1}\beta_i^2}{1-\overline{\alpha}_{i}^2}x_0 + \frac{\left(1-\overline{\alpha}_{i-1}^2\right)\alpha_i}{1-\overline{\alpha}_{i}^2}x_{i}\\
\sigma^2(i)&:=\frac{1-\overline{\alpha}_{i-1}^2}{1-\overline{\alpha}_{i}^2}\beta_i^2
\end{aligned}
\end{equation}
$$

下面我们证明这一点。



根据贝叶斯定理，有：

$$
\begin{equation}
p(x_{i-1}\mid x_i,x_0)=\frac{p(x_{i}\mid x_{i-1},x_0)p(x_{i-1}\mid x_0)}{p(x_i\mid x_0)}
\end{equation}
$$

由于前向过程具有马尔可夫性，即 $p(x_i\mid x_{i-1},x)=p(x_i\mid x_{i-1})$，于是：

$$
\begin{equation}
p(x_{i-1}\mid x_i,x_0)=\frac{p(x_{i}\mid x_{i-1})p(x_{i-1}\mid x_0)}{p(x_i\mid x_0)}
\end{equation}
$$

由于等式右侧的三个分布都是高斯分布，因此所求条件后验分布也是一个高斯分布。

又由于分母的 $p(x_i\mid x_0)$ 与 $x_{i-1}$ 无关，因此我们只需要计算 $p(x_{i}\mid x_{i-1})p(x_{i-1}\mid x_0)$ 的均值和方差即可。

为了简化符号，我们令 $x=x_{i-1}$。由定义，我们有：

$$
\begin{equation}
\begin{aligned}
p(x_i\mid x)&=\mathcal{N}(x_i;\alpha_i x,\beta_i^2I)\\
p(x\mid x_0)&=\mathcal{N}(x;\overline{\alpha}_{i-1}x_0,\left(1-\overline{\alpha}_{i-1}^2\right)I)
\end{aligned}
\end{equation}
$$

令 $\mu_0=\overline{\alpha}_{i-1}x_0$，$\Sigma_0=\left(1-\overline{\alpha}_{i-1}^2\right)I$，则我们有：

$$
\begin{equation}
p(x_i\mid x)p(x\mid x_0)
\propto
\exp\left( -\frac12\left[ (x_i-\alpha_ix)^T\left( \beta_i^2I \right)^{-1}(x_i-\alpha_ix)+(x-\mu_0)^T\Sigma_0^{-1}(x-\mu_0) \right] \right)
\end{equation}
$$

我们仅考虑上式的指数部分，在忽略常数项的情况下，我们对指数部分进行化简得到：

$$
\begin{equation}
 -\frac12\left[ \frac{1}{\beta_i^2}\|x_i-\alpha_ix\|^2 +\frac{1}{1-\overline{\alpha}_{i-1}^2}\|x-\mu_0\|^2 \right]
\end{equation}
$$

展开平方项：

$$
\begin{equation}
\begin{aligned}
\|x_i-\alpha_ix\|^2
&=x_i^Tx_i-2\alpha_ix_i^Tx+\alpha_i^2x^Tx\\
\|x-\mu_0\|^2
&=x=^Tx-2\mu_0^Tx+\mu_0^T\mu_0\\
\end{aligned}
\end{equation}
$$

我们仅保留与 $x$ 有关的项，并代入上式得：

$$
\begin{equation}
 -\frac12\left[ \left(\frac{\alpha_i^2}{\beta_i^2}+\frac{1}{1-\overline{\alpha}_{i-1}^2}\right) x^Tx - 2 \left(\frac{\alpha_i}{\beta_i^2}x_i+\frac{\mu_0}{1-\overline{\alpha}_{i-1}^2}\right)^Tx \right]
\end{equation}
$$

> **Remark.** 考虑多元高斯分布：
> $$
> p(x)
> \propto
> \exp\left( -\frac12(x-\mu)^T\Sigma^{-1}(x-\mu) \right)
> $$
> 将指数部分展开为二次型得：
> $$
> (x-\mu)^T\Sigma^{-1}(x-\mu)=x^T\Sigma^{-1}x-2\mu^T\Sigma^{-1}x+\mu^T\Sigma^{-1}\mu
> $$
> 因此，任意形如：
> $$
> \exp\left( -\frac12x^TAx+b^Tx \right)
> $$
> 的表达式都对应着一个高斯分布，其中：
>
> - 精度矩阵（拟协方差）为：$A=\Sigma^{-1}$；
> - 均值满足：$A\mu=b$，即 $\mu=A^{-1}b$。

这正好是一个高斯分布，其精度矩阵为：

$$
\begin{equation}
\Lambda = \left(\frac{\alpha_i^2}{\beta_i^2}+\frac{1}{1-\overline{\alpha}_{i-1}^2}\right)I
\end{equation}
$$

均值为：

$$
\begin{equation}
\mu=\Lambda^{-1}\left(\frac{\alpha_i}{\beta_i^2}x_i+\frac{\mu_0}{1-\overline{\alpha}_{i-1}^2}\right)
\end{equation}
$$

首先我们计算协方差矩阵，我们有：

$$
\begin{equation}
\begin{aligned}
A
&:=\frac{\alpha_i^2}{\beta_i^2}+\frac{1}{1-\overline{\alpha}_{i-1}^2}\\
&=\frac{\left(1-\overline{\alpha}_{i-1}^2\right)\alpha_i^2+\beta_i^2}{\beta_i^2\left(1-\overline{\alpha}_{i-1}^2\right)}\\
&=\frac{\alpha_i^2+\beta_i^2-\overline{\alpha}_{i-1}^2\alpha_i^2}{\beta_i^2\left(1-\overline{\alpha}_{i-1}^2\right)}\\
&=\frac{1-\overline{\alpha}_{i}^2}{\beta_i^2\left(1-\overline{\alpha}_{i-1}^2\right)}\\
\end{aligned}
\end{equation}
$$

因此，协方差矩阵为：

$$
\begin{equation}
\sigma^2(i)=\frac1A=\frac{1-\overline{\alpha}_{i-1}^2}{1-\overline{\alpha}_{i}^2}\beta_i^2
\end{equation}
$$

然后我们计算均值。我们有：

$$
\begin{equation}
\begin{aligned}
\mu(x_i,x_0,i)
&=\frac{1}{A}\left(\frac{\alpha_i}{\beta_i^2}x_i+\frac{\mu_0}{1-\overline{\alpha}_{i-1}^2}\right)\\
&=\frac{\beta_i^2\left(1-\overline{\alpha}_{i-1}^2\right)}{1-\overline{\alpha}_{i}^2}\left(\frac{\alpha_i}{\beta_i^2}x_i+\frac{\overline{\alpha}_{i-1}x_0}{1-\overline{\alpha}_{i-1}^2}\right)\\
&=\frac{\beta_i^2\overline{\alpha}_{i-1}}{1-\overline{\alpha}_{i}^2}x_0+\frac{\left(1-\overline{\alpha}_{i-1}^2\right)\alpha_i}{1-\overline{\alpha}_{i}^2}x_i\\
\end{aligned}
\end{equation}
$$

得证。



### 公式 (20) 的证明

下面我们证明均值表达式可以写为：

$$
\begin{equation}
\begin{aligned}
\mu(x_i,x_0,i)
&:=\frac{\overline{\alpha}_{i-1}\beta_i^2}{1-\overline{\alpha}_{i}^2}x_0 + \frac{\left(1-\overline{\alpha}_{i-1}^2\right)\alpha_i}{1-\overline{\alpha}_{i}^2}x_{i}\\
&=\frac{1}{\alpha_i}\left( x_i-\frac{1-\alpha_i^2}{\sqrt{1-\overline{\alpha}_i^2}}\epsilon \right)
\end{aligned}
\end{equation}
$$


将 $x_i=\overline{\alpha}_i x_0+\sqrt{1-\overline{\alpha}_i^2}\epsilon$ 代入得：

$$
\begin{equation}
\begin{aligned}
\mu(x_i,x_0,i)
&:=\frac{\overline{\alpha}_{i-1}\beta_i^2}{1-\overline{\alpha}_{i}^2}x_0 + \frac{\left(1-\overline{\alpha}_{i-1}^2\right)\alpha_i}{1-\overline{\alpha}_{i}^2}x_{i}\\
&=\frac{\overline{\alpha}_{i-1}\beta_i^2}{1-\overline{\alpha}_{i}^2}\frac{x_i-\sqrt{1-\overline{\alpha}_i^2}\epsilon}{\overline{\alpha}_i} + \frac{\left(1-\overline{\alpha}_{i-1}^2\right)\alpha_i}{1-\overline{\alpha}_{i}^2}x_{i}\\
&=\frac{\beta_i^2}{\left(1-\overline{\alpha}_{i}^2\right)\alpha_i}\left(x_i-\sqrt{1-\overline{\alpha}_i^2}\epsilon\right) + \frac{\left(1-\overline{\alpha}_{i-1}^2\right)\alpha_i}{1-\overline{\alpha}_{i}^2}x_{i}\\
&=\left(\frac{\beta_i^2}{\left(1-\overline{\alpha}_{i}^2\right)\alpha_i} + \frac{\left(1-\overline{\alpha}_{i-1}^2\right)\alpha_i}{1-\overline{\alpha}_{i}^2}\right)x_i-\frac{\beta_i^2}{\alpha_i\sqrt{1-\overline{\alpha}_i^2}}\epsilon
\end{aligned}
\end{equation}
$$

其中，$x_i$ 前的系数可以化简为：

$$
\begin{equation}
\begin{aligned}
\frac{\beta_i^2}{\left(1-\overline{\alpha}_{i}^2\right)\alpha_i} + \frac{\left(1-\overline{\alpha}_{i-1}^2\right)\alpha_i}{1-\overline{\alpha}_{i}^2}
&=\frac{\beta_i^2+\left(1-\overline{\alpha}_{i-1}^2\right)\alpha_i^2}{\left(1-\overline{\alpha}_{i}^2\right)\alpha_i}\\
&=\frac{1-\overline{\alpha}_{i}^2}{\left(1-\overline{\alpha}_{i}^2\right)\alpha_i}\\
&=\frac{1}{\alpha_i}
\end{aligned}
\end{equation}
$$

代入得：

$$
\begin{equation}
\begin{aligned}
\mu(x_i,x_0,i)
&=\frac{1}{\alpha_i}x_i-\frac{\beta_i^2}{\alpha_i\sqrt{1-\overline{\alpha}_i^2}}\epsilon\\
&=\frac{1}{\alpha_i}\left( x_i-\frac{1-\alpha_i^2}{\sqrt{1-\overline{\alpha}_i^2}}\epsilon \right)
\end{aligned}
\end{equation}
$$

得证。

### Proof on Theorem 2.

定理2描述了DDPM中ELBO的形式，下面我们来证明这个ELBO是对数概率密度 $\log p_\phi(x)$ 的下界估计。

**Step 1: 通过Jensen不等式写出ELBO的形式**

由定义，我们可以写出联合分布和边缘分布的形式：

$$
\begin{equation}
\begin{aligned}
p_\phi(x,x_{0:L})
&:=p_{prior}(x_L)\prod_{i=1}^L p_\phi(x_{i-1}\mid x_i)\cdot p_\phi(x\mid x_0) \\
\log p_\phi(x)
&:=\log \int p_\phi(x,x_{0:L})\mathrm{d}x_{0:L}
\end{aligned}
\end{equation}
$$

我们引入一个变分分布 $p(x_{0:L}\mid x)$ ，并应用Jensen不等式得：

$$
\begin{equation}
\begin{aligned}
\log p_\phi(x)
&:=\log \int p_\phi(x,x_{0:L})\mathrm{d}x_{0:L}\\
&=\log \int p(x_{0:L}\mid x)\frac{ p_\phi(x,x_{0:L})}{p(x_{0:L}\mid x)}\mathrm{d}x_{0:L}\\
&=\log\mathbb{E}_{p(x_{0:L}\mid x)}\left[\frac{ p_\phi(x,x_{0:L})}{p(x_{0:L}\mid x)}\right]\\
&\ge \mathbb{E}_{p(x_{0:L}\mid x)}\left[\log\frac{ p_\phi(x,x_{0:L})}{p(x_{0:L}\mid x)}\right]\\
&:=\mathcal{L}_{ELBO}
\end{aligned}
\end{equation}
$$

**Step 2: 分解ELBO的形式**

我们将变分分布 $p(x_{0:L}\mid x)$ 拆分为下面的形式：

$$
\begin{equation}
p(x_{0:L}\mid x)=p(x_L\mid x)\prod_{i=1}^L p(x_{i-1}\mid x_i,x)
\end{equation}
$$

代入ELBO中得：

$$
\begin{equation}
\begin{aligned}
\mathcal{L}_{ELBO}
&:= \mathbb{E}_{p(x_{0:L}\mid x)}\left[\log\frac{ p_\phi(x,x_{0:L})}{p(x_{0:L}\mid x)}\right]\\
&= \mathbb{E}_{p(x_{0:L}\mid x)}\left[ \log p_{prior}(x_L)+\sum_{i=1}^L \log p_\phi(x_{i-1}\mid x_i) +\log p_{\phi}(x\mid x_0)-\log p(x_L\mid x)-\sum_{i=1}^L \log p(x_{i-1}\mid x_i,x)\right]
\end{aligned}
\end{equation}
$$

进行一些合并得：

$$
\begin{equation}
\begin{aligned}
-\mathcal{L}_{ELBO}
=& \mathbb{E}_{p(x_0\mid x)}\left[ -\log p_\phi(x\mid x_0) \right]
+\mathbb{E}_{p(x_L\mid x)}\left[ \log\frac{p(x_L\mid x)}{p_{prior}(x_L)} \right]\\
&+\sum_{i=1}^L\mathbb{E}_{p(x_i\mid x)}\left[ \mathbb{E}_{p(x_{i-1}\mid x_i,x)}\left[ \log\frac{p(x_{i-1}\mid x_i,x)}{p_\phi(x_{i-1}\mid x_i)} \right] \right]
\end{aligned}
\end{equation}
$$

这第一项对应 $\mathcal{L}_{\text{recon}}$，第二项对应 $\mathcal{L}_{\text{prior}}$ 。

我们将最后一项做一些变形：

$$
\begin{equation}
\begin{aligned}
\mathbb{E}_{p(x_i\mid x)}\left[ \mathbb{E}_{p(x_{i-1}\mid x_i,x)}\left[ \log\frac{p(x_{i-1}\mid x_i,x)}{p_\phi(x_{i-1}\mid x_i)} \right] \right]
&=\int p(x_i\mid x)\left[ \int p(x_{i-1}\mid x_i,x)\log\frac{p(x_{i-1}\mid x_i,x)}{p_\phi(x_{i-1}\mid x_i)}\mathrm{d}x_{i-1}  \right]\mathrm{d}x_i\\
&=\mathbb{E}_{p(x_{0:L}\mid x)}\left[\log\frac{p(x_{i-1}\mid x_i,x)}{p_\phi(x_{i-1}\mid x_i)}\right]
\end{aligned}
\end{equation}
$$

因此，第三项正好就是 $\mathcal{L}_\text{diffusion}$ 的形式。

得证。